# 632

## Question

Given an array `nums` of `k` non-decreasing arrays, find the shortest interval `[s, e]` such that each array in `nums` has at least one element in the range `[s, e]`. If there are multiple such intervals of the same length (`e - s`), find the one with the smallest left-endpoint (`s`).

## Solution

Instead of `k` independent arrays, we can consider one long array where each element has a value and one of `k` tags. The original problem is then to find the shortest interval where every tag appears at least once. This can be done with classic sliding window, where we expand the window to the right by adding one more element, then shrink the window from the left by removing elements while keeping the window valid.

The classic way to construct such long array is to consider merging of these `k` arrays, so a heap to track the next smallest element and the array it is from, a list of indices to track how many elements remain in each array, and (to track how the window should change) a deque to track elements currently in the window.

However, in the worse case (e.g. one of the arrays contains only one element, which is larger than everything else) the deque would contain every element from all the arrays, so space-wise it is not too different from joining all arrays before sorting. Furthermore, the overhead from maintaining the heap and the queue may become dominant on small cases (when long array is manageable, joining and sorting requires only simple array access). In fact, joining and sorting does appears to be faster than the merging approach by quite a margin, possibly due to the small input size (`k <= 3500`, `len(nums[i]) <= 50`).

